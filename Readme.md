My 100 Days of Machine Learning Journey with CampusX
ğŸš€ Welcome to My Repository!
This repository is my personal journal and code sanctuary for the 100 Days of Machine Learning course by CampusX. I've embarked on this journey to build a strong foundation in Machine Learning and Data Science, from the ground up. Here, I'll be documenting my daily progress, sharing all the code I write, and summarizing my key learnings.

Follow along with my journey!

ğŸ‘¨â€ğŸ« About the Course
This incredible course is curated by Gaurav Sir (CampusX). The curriculum is designed to cover everything from Python basics for ML to advanced topics like Deep Learning, with a strong focus on practical implementation and intuition behind the algorithms. My goal is to diligently follow the lectures, complete the assignments, and build a portfolio of projects.

ğŸ’» Technologies & Libraries I'm Mastering
Language: Python

Libraries:

Data Manipulation & Analysis: NumPy, Pandas

Data Visualization: Matplotlib, Seaborn

Machine Learning: Scikit-learn

Web Scraping: BeautifulSoup

Tools: Jupyter Notebook, Google Colab, Git & GitHub

ğŸ—“ï¸ My Daily Journey Log
Here is a log of my day-to-day progress. Each entry includes the topics covered, a link to the code/notebook for that day, and my personal notes and key takeaways.

Day

Topic Covered

Code / Notebook Link

Key Learnings & Notes

...

...

...

...

015

Working with CSV files

Day 15 Notebook

- Learned to load CSVs from local files and URLs.<br>- Explored various pd.read_csv parameters like sep, names, index_col, skiprows, and encoding.<br>- Handled large datasets efficiently using chunksize.

016

Working with JSON and SQL

Day 16 Notebook

- Practiced reading JSON files into DataFrames (pd.read_json).<br>- Understood how to handle different JSON structures and orientations.<br>- Learned to query SQL databases directly into a Pandas DataFrame.

017

Fetching API data into a DataFrame

Day 17 Notebook

- Used the requests library to make API calls.<br>- Parsed JSON responses from APIs.<br>- Built a complete DataFrame by fetching data from multiple API pages.

018

Web Scraping for DataFrames

Day 18 Notebook

- Mastered the basics of web scraping using requests and BeautifulSoup.<br>- Learned to parse HTML and find specific elements by their tags.<br>- Successfully extracted data from a website and structured it into a DataFrame.


ğŸ› ï¸ Projects
This section showcases the major projects I've built during this 100-day journey.

Project 1: Titanic Survival Prediction
Description: A classic classification problem to predict whether a passenger survived the Titanic disaster based on features like age, class, and sex.



(...more projects to be added as I build them!)

ğŸ”— Connect with Me
I'm always open to connecting with fellow learners and professionals. Feel free to reach out!

LinkedIn: 

GitHub: Ishant57

Twitter: ishantbhoyar7

ğŸ™ Acknowledgements
A huge thank you to Gaurav Sir and the entire CampusX team for creating this invaluable free resource for the community. This journey wouldn't be possible without their dedication to teaching.